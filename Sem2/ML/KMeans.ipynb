{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "741368a1",
   "metadata": {},
   "source": [
    "# KMeans\n",
    "1. It's a Unsupervised Learning algorithm used for grouping like objects together.\n",
    "2. E.g. Data Segmentation(Find k regions where a product's users live), Fraud Detection, MNIST without labels, Light classification.\n",
    "3. Algorithm :\n",
    "<img src=\"./images/kmeans.png\" width=\"50%\" height = \"50%\"></img>\n",
    "4. Inputs:\n",
    "    * K(number of clusters)\n",
    "    * Features which are not real numbers(eg. categorical features) won't work\n",
    "5. Process :\n",
    "    * Initialisation may use data points as initial centroids\n",
    "    * Distance is always Euclidean\n",
    "    * The running time of K-means is O($n^{kd+1}$) in d-dimensional space.\n",
    "6. Output :\n",
    "    * Labels of instances\n",
    "    * Location of cluster centroids\n",
    "7. Point of convergence\n",
    "    * None of the elements move in the centroid\n",
    "    * None of the centroids move\n",
    "    * Number of iterations<br>\n",
    "    * scikit-learn: k-means clustered at 273.\n",
    "[**Note : if the data is moved from n-d to (n+1)d, then the distance between the points increases and thus creates challenge to the clusters**]\n",
    "8. Reduce inertia :\n",
    "    * within-clusters sum-of-squares\n",
    "    * lower values are better.\n",
    "    * with Euclidean distances, **\"curse of dimensionality\"(when we are reducing high dimension to low then we might miss out some features that are visible at high-dimensional space but not in lower dimension)** may be a problem\n",
    "9. kMeans sklearn:\n",
    "    * `sklearn.clusters.KMeans`\n",
    "    * `kmeans = KMeans(n_clusters=8, n_init=10, max_iter=300, tol=0.0001)`, where n_init : sklearn will execute k-means algorithm this many times.\n",
    "10. Problems with starting points\n",
    "    <img src=\"./images/clusterCenter.png\" width=\"50%\" height = \"50%\"></img>\n",
    "    * Thus solution is not to start with random data points!!!\n",
    "    * Solution **Farthest first, k-means++(also randomly with probability distance)**\n",
    "11. Evaluating results :\n",
    "    * Optimise to dense clusters\n",
    "    * Calculate cluster density by sum of sqaured distance of points from centroid\n",
    "    * Elbow method : \n",
    "        * Try with varying \"k\" values\n",
    "        * Find an \"elbow\" after which density of clusters starts to diminish.\n",
    "        <img src=\"./images/elbow.png\" width=\"50%\" height = \"50%\"></img>\n",
    "    * without lables/ ground truth:\n",
    "        * Silhouette coefficient - measure of cluster compactness.\n",
    "    * With Labels/ Ground Truth:\n",
    "        * Homogeneity(H) : fractional \"purity\" of all clusters\n",
    "        * Completeness (C) : all class members in one cluster\n",
    "        * V-Measure:\n",
    "            * Penalty for [over]-generating clusters\n",
    "            * Penalty for generating impure clusters (2*HC/H+C)\n",
    "12. Effect of Outlier:\n",
    "    * KMeans is sensitive to outliers in a sense that it tries to pull away the cluster center outwards.\n",
    "    * Thus mean **is not a robust statistics** for measurement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8945822f",
   "metadata": {},
   "source": [
    "# Clustering Algorithms\n",
    "1. Disadvantage of kMeans :\n",
    "    - Cluster boundaries must be relatively simple.\n",
    "    - Will always generate clusters(even if none exist)\n",
    "    - Assume all dimensions are equal\n",
    "2. Other Clustring Algorithm:\n",
    "    - DBSCAN and KMeans seeks compactness\n",
    "    - Spectral Clustering seeks connectivity\n",
    "    - Hierarchical also seeks Connectivity.\n",
    "3. **DBSCAN**\n",
    "    - Density-based spatial Clustering with Noise \n",
    "        * A set of points form a cluster if in dense neighbourhood\n",
    "        * Starts with a random point and covers all the points within the 'eps' distance!\n",
    "        * Hyperparamaters :\n",
    "            * eps : max distance b/w 2 points in the same neighbourhood\n",
    "            * min_samples : number of observations to form a neighbourhood\n",
    "        * [Naftali's Animation](https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/)\n",
    "    - 3 points:\n",
    "        * Core point\n",
    "        * Reachable point\n",
    "        * Qutlier point\n",
    "        <img src=\"./images/dbscan1.png\" width=\"50%\" height = \"50%\" align=\"center\"></img>\n",
    "    - Advantage :\n",
    "        * Can find clusters of any shape - great at geolocation data\n",
    "        * Some points may not belong to any clusters\n",
    "        * No need to specify number of clusters\n",
    "    - Disadvantage :\n",
    "        * Curse of Dimensionality\n",
    "        * Ideal eps is difficult to get right\n",
    "        * Densities are defined globally, so sparse neighbourhoods are difficult to find.\n",
    "4. **Hierarchical Clustering**\n",
    "    - Builds a **dendogram** over a data\n",
    "    <img src=\"./images/dendogram.png\" width=\"50%\" height = \"50%\" align=\"center\"></img>\n",
    "    - Input = feature vector(ideally: normalize features)\n",
    "    - Algorithm :\n",
    "        * Start with each observation in its own cluster\n",
    "        * Repeat until it convergence:\n",
    "            * Merge 2 closest clusters\n",
    "            * Converge when only one cluster remians\n",
    "        <img src=\"./images/h1.png\" width=\"50%\" height = \"50%\" align=\"center\"></img>\n",
    "    - Output : order of instance merges\n",
    "        * 'D','E' almost as far as 'A','C'.\n",
    "        * Height tells when the objects were clustered together(smaller - earlier).\n",
    "    - Linkage Types\n",
    "        * Average : avg distance between each point between given clusters, Considers all 6 values.\n",
    "        * Complete : Record Largest pairwise inter-cluster dissimilarity\n",
    "        * Ward : Sum of squared distances between all observations between clusters, Consider all 6 values\n",
    "        <img src=\"./images/h2.png\" width=\"50%\" height = \"50%\" align=\"center\"></img>\n",
    "    - Distance between Clusters:\n",
    "        * Euclidean Distance (ward)\n",
    "        * In sklearn, change `affinity`:\n",
    "            * Cosine(Hamming distance)\n",
    "            * L1 **(used in high-dimensional dataset)**\n",
    "            * L2 (same as euclidean Distance)\n",
    "            * Manhattan (same as L1)\n",
    "        * Normalizing data is useful\n",
    "        * sklearn imp : \n",
    "        <img src=\"./images/h3.png\" width=\"50%\" height = \"50%\" align=\"center\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fadcf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
