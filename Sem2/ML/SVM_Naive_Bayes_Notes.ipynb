{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5540a13",
   "metadata": {},
   "source": [
    "# Support Vector Machines(SVM)\n",
    "1. Pros:\n",
    "    * Memory and computationally inexpensive bcoz it uses a subset of data.\n",
    "    * Easy to interpret results\n",
    "    * Makes few errors when there are clear.\n",
    "    * Effective when dimensions are large, effective to high dimensions.\n",
    "2. Cons:\n",
    "    * Sensitive to configuration and outliers - needs small-ish dataset.\n",
    "    * Natively sensitive to class imbalances, noisy data.\n",
    "    * Inherently expects pairs of classes.\n",
    "3. Can be used for both Classification/ Regression task.\n",
    "4. Goal : Find a hyperplanes, find the one with the largest width(Maximum Margin Classifier).\n",
    "    * <img src=\"images/svm1.png\" width = \"50%\" height = \"50%\"/>, Selects the 2nd hyperplane for separation.\n",
    "5. What's considered:\n",
    "    * In support vectors classifiers only support vectors.\n",
    "6. Slack Variables:\n",
    "    * 'c' puts weightage of how much errors is allowed.\n",
    "    * SMO (Sequential Minimal Optimisation)\n",
    "    * Harrington's implementation on github.\n",
    "    * <img src=\"images/svm2.png\" width = \"50%\" height = \"50%\"/>\n",
    "    * Not linearly separable,hence find new feature set that can be linearly separable.\n",
    "7. When classes are(nearly) separable, SVM generally performs better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bd8b50",
   "metadata": {},
   "source": [
    "# Naive Bayes:\n",
    "1. It is a generative model(bunch of classes, if I've class 'x' distribution is like this -> opps of this).\n",
    "2. Application :\n",
    "    * Used in conjunction with collaborative filtering\n",
    "    * Spam filtering\n",
    "    * Sentiment Analysis\n",
    "3. Adv :\n",
    "    * Well-understood and easy to implement.\n",
    "    * Can perform multi-class classification and can do quickly.\n",
    "    * Can use categorical as well as numeric features.\n",
    "    * Needs fewer training data for training.\n",
    "4. Disadv : \n",
    "    * Given classes & feature can have zero frequency, leading to incorrect probability estimates can be fixed with smoothing techniques such as Laplace Estimation, etc.\n",
    "    * Features are assumed to be independent, which may not be the case\n",
    "5. Formula :\n",
    "    * <img src=\"images/nb1.png\" width = \"50%\" height = \"50%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecbd76d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
